# üèóÔ∏è Architecture Overview

> A production-ready Kubernetes cluster on Ubuntu 22.04, fully automated with Ansible, featuring monitoring, runtime security, and CIS-hardened infrastructure ‚Äî optimized for an 8GB RAM, 2-node lab environment.

---

## üìÅ Project Structure

```
Cdac Project/
‚îú‚îÄ‚îÄ ansible/                         # Infrastructure Automation
‚îÇ   ‚îú‚îÄ‚îÄ inventory/hosts.ini          # VM IP addresses (master + worker)
‚îÇ   ‚îú‚îÄ‚îÄ group_vars/all.yml           # Global configuration variables
‚îÇ   ‚îú‚îÄ‚îÄ site.yml                     # Main orchestration playbook (4 plays)
‚îÇ   ‚îî‚îÄ‚îÄ roles/
‚îÇ       ‚îú‚îÄ‚îÄ common/                  # OS prep, containerd, K8s packages
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ tasks/main.yml       # Swap, sysctl, containerd, kubelet
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ handlers/main.yml    # Service restart handlers
‚îÇ       ‚îú‚îÄ‚îÄ k8s_master/              # Control plane initialization
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ tasks/main.yml       # kubeadm init, CNI, PSS, etcd backup
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ handlers/main.yml    # Service restart handlers
‚îÇ       ‚îú‚îÄ‚îÄ k8s_worker/              # Worker node cluster join
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ tasks/main.yml       # Join command, node readiness
‚îÇ       ‚îî‚îÄ‚îÄ security/                # Host-level hardening
‚îÇ           ‚îú‚îÄ‚îÄ tasks/main.yml       # UFW, SSH, CIS benchmarks
‚îÇ           ‚îî‚îÄ‚îÄ handlers/main.yml    # SSH/kubelet restart handlers
‚îú‚îÄ‚îÄ kubernetes/                      # K8s Manifests (applied by Ansible)
‚îÇ   ‚îú‚îÄ‚îÄ monitoring/                  # Observability Stack
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ namespace.yaml           # monitoring namespace (PSS: baseline)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus.yaml          # Deployment + RBAC + ConfigMap + Service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ prometheus-alerts.yaml   # Alert rules ConfigMap
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana.yaml             # Deployment + Datasource ConfigMap + Service
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ grafana-dashboards.yaml  # Pre-built dashboard JSON ConfigMaps
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ kube-state-metrics.yaml  # Deployment + RBAC + Service
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ node-exporter.yaml       # DaemonSet + Service
‚îÇ   ‚îú‚îÄ‚îÄ nginx/                       # Sample Workload
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ deployment.yaml          # Deployment + Service (PSS-compliant)
‚îÇ   ‚îú‚îÄ‚îÄ security/                    # Cluster Security Policies
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ network-policy.yaml      # Default-deny + allow rules
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ pss-rbac.yaml            # PSS labels + RBAC role/binding
‚îÇ   ‚îú‚îÄ‚îÄ storage/                     # Persistent Storage (NFS)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ storage-class.yaml       # nfs-storage StorageClass
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ nfs-pv.yaml              # 4 PersistentVolumes (10Gi+5Gi+2Gi+1Gi)
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ nfs-pvc.yaml             # 4 PersistentVolumeClaims
‚îÇ   ‚îî‚îÄ‚îÄ falco/                       # Runtime Security (optional)
‚îÇ       ‚îî‚îÄ‚îÄ falco.yaml               # Namespace + RBAC + ConfigMap + DaemonSet
‚îú‚îÄ‚îÄ scripts/                         # Operational Scripts
‚îÇ   ‚îú‚îÄ‚îÄ etcd-backup.sh               # Automated hourly etcd snapshot + NFS copy
‚îÇ   ‚îî‚îÄ‚îÄ diagnose-services.sh         # Cluster health diagnostic report
‚îî‚îÄ‚îÄ docs/                            # Documentation
    ‚îú‚îÄ‚îÄ Kubernetes_Cluster_Project_Document.md
    ‚îú‚îÄ‚îÄ Project_Explanation.md
    ‚îú‚îÄ‚îÄ Interview_QA_Guide.md
    ‚îú‚îÄ‚îÄ Updated_Interview_QA.md
    ‚îú‚îÄ‚îÄ interview_extra.md
    ‚îî‚îÄ‚îÄ setup_guide.md
```

---

## üåê High-Level Architecture

```mermaid
graph TB
    subgraph CONTROL["üñ•Ô∏è Ansible Control Machine"]
        A["ansible-playbook site.yml"]
    end

    subgraph INFRA["üè¢ Infrastructure Layer (Ubuntu 22.04 VMs)"]
        subgraph MASTER["Master Node (192.168.144.130)"]
            API["kube-apiserver :6443"]
            ETCD["etcd :2379"]
            SCHED["kube-scheduler"]
            CM["kube-controller-manager"]
            KUBELET_M["kubelet"]
        end
        subgraph WORKER["Worker Node (192.168.144.134)"]
            KUBELET_W["kubelet"]
            KPROXY["kube-proxy"]
        end
        subgraph NFS_SRV["NFS Server (192.168.144.132)"]
            NFS_SHARE["/srv/nfs/kubernetes"]
        end
    end

    subgraph K8S["‚ò∏Ô∏è Kubernetes Cluster (v1.29)"]
        subgraph FLANNEL["Flannel CNI (10.244.0.0/16)"]
            direction LR
        end
        subgraph NS_MON["Namespace: monitoring"]
            PROM["Prometheus :30090"]
            GRAF["Grafana :30300"]
            NE["Node Exporter (DaemonSet)"]
            KSM["Kube-State-Metrics"]
        end
        subgraph NS_DEF["Namespace: default"]
            NGINX["Nginx x2 :30080"]
        end
        subgraph NS_FALCO["Namespace: falco"]
            FALCO["Falco (DaemonSet)"]
        end
    end

    A -->|"Play 1: common + security"| MASTER
    A -->|"Play 1: common + security"| WORKER
    A -->|"Play 2: k8s_master"| MASTER
    A -->|"Play 3: k8s_worker"| WORKER
    A -->|"Play 4: Deploy manifests"| K8S

    PROM -->|scrape :9100| NE
    PROM -->|scrape :8080| KSM
    PROM -->|scrape :8765| FALCO
    GRAF -->|query :9090| PROM
    PROM --> NFS_SHARE
    GRAF --> NFS_SHARE

    classDef master fill:#4a90d9,stroke:#2d5986,color:#fff
    classDef worker fill:#5cb85c,stroke:#3d8b3d,color:#fff
    classDef nfs fill:#f0ad4e,stroke:#c87f0a,color:#000
    classDef monitoring fill:#9b59b6,stroke:#6c3483,color:#fff
    classDef security fill:#e74c3c,stroke:#a93226,color:#fff

    class API,ETCD,SCHED,CM,KUBELET_M master
    class KUBELET_W,KPROXY worker
    class NFS_SRV,NFS_SHARE nfs
    class PROM,GRAF,NE,KSM monitoring
    class FALCO security
```

---

## ‚öôÔ∏è Ansible Automation Flow

The `site.yml` playbook orchestrates the entire deployment in **4 sequential plays**:

```mermaid
flowchart TD
    START(["‚ñ∂ ansible-playbook site.yml"]) --> P1

    subgraph P1["Play 1 ‚Äî All Nodes"]
        direction TB
        P1A["Disable swap"] --> P1B["Install packages<br/>(apt-transport-https, nfs-common, ...)"]
        P1B --> P1C["Load kernel modules<br/>(overlay, br_netfilter)"]
        P1C --> P1D["Configure sysctl<br/>(ip_forward, bridge-nf-call)"]
        P1D --> P1E["Install containerd<br/>(SystemdCgroup = true)"]
        P1E --> P1F["Install kubelet,<br/>kubeadm, kubectl v1.29"]
        P1F --> P1G["UFW Firewall Rules<br/>(SSH, 6443, 10250, 30000-32767)"]
        P1G --> P1H["SSH Hardening<br/>(root login, password auth)"]
        P1H --> P1I["CIS Benchmarks<br/>(file perms, kubelet config)"]
    end

    P1 --> P2

    subgraph P2["Play 2 ‚Äî Master Only"]
        direction TB
        P2A["kubeadm init<br/>(--pod-network-cidr=10.244.0.0/16)"] --> P2B["Setup kubectl<br/>(root + ubuntu users)"]
        P2B --> P2C["Install Flannel CNI"]
        P2C --> P2D["Remove NoSchedule taint<br/>(allow workloads on master)"]
        P2D --> P2E["Apply Pod Security Standards<br/>(baseline enforcement)"]
        P2E --> P2F["Generate join command"]
        P2F --> P2G["Setup etcd backup cron<br/>(hourly ‚Üí /backup/etcd)"]
    end

    P2 --> P3

    subgraph P3["Play 3 ‚Äî Workers Only"]
        direction TB
        P3A["Get join command<br/>from master"] --> P3B["kubeadm join"]
        P3B --> P3C["Wait for node Ready"]
    end

    P3 --> P4

    subgraph P4["Play 4 ‚Äî Deploy K8s Services"]
        direction TB
        P4A["Copy manifests<br/>to /opt/kubernetes/"] --> P4B["Apply storage/<br/>(StorageClass, PV, PVC)"]
        P4B --> P4C["Apply monitoring/<br/>(Prometheus, Grafana, etc.)"]
        P4C --> P4D["Apply nginx/<br/>(Deployment + Service)"]
        P4D --> P4E["Apply security/<br/>(NetworkPolicy, RBAC)"]
        P4E --> P4F{"enable_falco?"}
        P4F -->|true| P4G["Apply falco/"]
        P4F -->|false| P4H["Skip Falco"]
        P4G --> DONE
        P4H --> DONE
    end

    DONE(["‚úÖ Cluster Ready"])

    style P1 fill:#e8f4f8,stroke:#5dade2
    style P2 fill:#fdebd0,stroke:#f39c12
    style P3 fill:#d5f5e3,stroke:#27ae60
    style P4 fill:#f5eef8,stroke:#8e44ad
```

---

## üîç Monitoring Architecture

```mermaid
graph LR
    subgraph TARGETS["Scrape Targets"]
        NE["Node Exporter<br/>:9100<br/>(DaemonSet)"]
        KSM["Kube-State-Metrics<br/>:8080"]
        KUBELET["Kubelet<br/>/metrics"]
        APISVR["API Server<br/>:6443/metrics"]
        FALCO["Falco<br/>:8765/metrics"]
        NGINX["Nginx Pods<br/>:8080"]
    end

    PROM["Prometheus<br/>:9090 ‚Üí NodePort :30090<br/>Retention: 2d / 500MB"]

    GRAF["Grafana<br/>:3000 ‚Üí NodePort :30300<br/>admin / admin"]

    ALERTS["Alert Rules<br/>(prometheus-alerts.yaml)"]

    DASH["Pre-built Dashboards<br/>‚Ä¢ Cluster Overview<br/>‚Ä¢ Node Metrics<br/>‚Ä¢ Pod Resources<br/>‚Ä¢ Falco Security"]

    NE -->|every 30s| PROM
    KSM -->|every 30s| PROM
    KUBELET -->|every 30s| PROM
    APISVR -->|every 30s| PROM
    FALCO -->|every 30s| PROM
    NGINX -.->|annotations| PROM

    ALERTS --> PROM
    PROM --> GRAF
    DASH --> GRAF

    style PROM fill:#e6522c,stroke:#c0392b,color:#fff
    style GRAF fill:#f9a825,stroke:#f57f17,color:#000
    style NE fill:#26a69a,stroke:#00897b,color:#fff
    style KSM fill:#42a5f5,stroke:#1565c0,color:#fff
    style FALCO fill:#ef5350,stroke:#c62828,color:#fff
```

---

## üîí Security Architecture

```mermaid
graph TB
    subgraph HOST["Host-Level Security (Ansible security role)"]
        FW["UFW Firewall<br/>Default: deny incoming<br/>Allow: SSH, 6443, 10250,<br/>30000-32767, 8472/UDP"]
        SSH["SSH Hardening<br/>No root login<br/>No password auth"]
        KERN["Kernel Hardening<br/>ASLR, rp_filter,<br/>no source routing"]
        CIS["CIS Benchmarks<br/>File perms 0600<br/>PKI cert/key perms<br/>Kubelet hardening"]
    end

    subgraph CLUSTER["Cluster-Level Security"]
        PSS["Pod Security Standards<br/>enforce: baseline<br/>warn: restricted<br/>audit: restricted"]
        RBAC["RBAC<br/>pod-reader Role<br/>developer RoleBinding"]
        NP["Network Policies<br/>default-deny-ingress<br/>allow-nginx-ingress<br/>allow-prometheus-scrape"]
    end

    subgraph RUNTIME["Runtime Security"]
        FALCO_R["Falco DaemonSet<br/>‚Ä¢ Shell in container<br/>‚Ä¢ Sensitive file access<br/>‚Ä¢ kubectl exec detection"]
        FALCO_R -->|metrics :8765| PROM_R["Prometheus<br/>(scrape + alert)"]
    end

    HOST --> CLUSTER --> RUNTIME

    style HOST fill:#ffebee,stroke:#c62828
    style CLUSTER fill:#e3f2fd,stroke:#1565c0
    style RUNTIME fill:#fce4ec,stroke:#ad1457
```

---

## üíæ Storage Architecture

```mermaid
graph TB
    subgraph NFS_SERVER["NFS Server (192.168.144.132)"]
        NFS1["/srv/nfs/kubernetes"]
        NFS2["/srv/nfs/kubernetes/prometheus"]
        NFS3["/srv/nfs/kubernetes/grafana"]
        NFS4["/srv/nfs/kubernetes/nginx"]
        NFS5["/srv/nfs/etcd-backups"]
    end

    subgraph STORAGE_CLASS["StorageClass: nfs-storage"]
        SC["Manual Provisioner<br/>WaitForFirstConsumer<br/>Retain policy"]
    end

    subgraph PVs["PersistentVolumes"]
        PV1["nfs-kubernetes-pv<br/>10Gi RWX"]
        PV2["nfs-prometheus-pv<br/>5Gi RWX"]
        PV3["nfs-grafana-pv<br/>2Gi RWX"]
        PV4["nfs-nginx-pv<br/>1Gi RWX"]
    end

    subgraph PVCs["PersistentVolumeClaims"]
        PVC1["nfs-pvc<br/>(default ns)"]
        PVC2["prometheus-pvc<br/>(monitoring ns)"]
        PVC3["grafana-pvc<br/>(monitoring ns)"]
        PVC4["nginx-pvc<br/>(default ns)"]
    end

    NFS1 --> PV1 --> PVC1
    NFS2 --> PV2 --> PVC2
    NFS3 --> PV3 --> PVC3
    NFS4 --> PV4 --> PVC4

    NFS5 -.->|etcd-backup.sh<br/>hourly cron| ETCD["etcd snapshots"]

    style NFS_SERVER fill:#fff3e0,stroke:#e65100
    style PVs fill:#e8f5e9,stroke:#2e7d32
    style PVCs fill:#e3f2fd,stroke:#1565c0
```

---

## üåê Network & Port Map

```mermaid
graph LR
    subgraph EXTERNAL["External Access"]
        USER["üë§ User / Browser"]
    end

    subgraph NODEPORTS["NodePort Services"]
        NP1[":30080 ‚Üí Nginx"]
        NP2[":30090 ‚Üí Prometheus"]
        NP3[":30300 ‚Üí Grafana"]
    end

    subgraph INTERNAL["Internal Cluster Network"]
        K8S_API[":6443 API Server"]
        ETCD_P[":2379-2380 etcd"]
        KUBELET_P[":10250 Kubelet"]
        FLANNEL_P[":8472/UDP Flannel VXLAN"]
        NE_P[":9100 Node Exporter"]
    end

    USER --> NP1
    USER --> NP2
    USER --> NP3

    style EXTERNAL fill:#e8eaf6,stroke:#283593
    style NODEPORTS fill:#f3e5f5,stroke:#6a1b9a
    style INTERNAL fill:#e0f2f1,stroke:#00695c
```

| Service | Type | Port | NodePort | Namespace |
|---------|------|------|----------|-----------|
| Nginx | NodePort | 80 ‚Üí 8080 | 30080 | default |
| Prometheus | NodePort | 9090 | 30090 | monitoring |
| Grafana | NodePort | 3000 | 30300 | monitoring |
| Node Exporter | ClusterIP | 9100 | ‚Äî | monitoring |
| Kube-State-Metrics | ClusterIP | 8080 | ‚Äî | monitoring |
| Falco | ClusterIP | 8765 | ‚Äî | falco |

---

## üîÑ Self-Healing & Reliability

```mermaid
graph TD
    subgraph PROBE["Health Probes"]
        NGINX_P["Nginx<br/>Liveness: GET / :8080<br/>every 5s, fail after 3<br/>Readiness: GET / :8080<br/>every 3s"]
        GRAF_P["Grafana<br/>Liveness: GET /api/health :3000<br/>every 10s<br/>Readiness: GET /api/health :3000<br/>every 5s"]
    end

    subgraph STRATEGY["Deployment Strategy"]
        RS["RollingUpdate<br/>maxSurge: 1<br/>maxUnavailable: 0"]
    end

    subgraph BACKUP["etcd Backup"]
        CRON["Cron: Every hour"] --> SNAP["etcdctl snapshot save"]
        SNAP --> LOCAL["/backup/etcd<br/>(keep 24h)"]
        SNAP --> REMOTE["NFS: /mnt/nfs/etcd-backups<br/>(keep 7 days)"]
    end

    style PROBE fill:#e8f5e9,stroke:#2e7d32
    style STRATEGY fill:#fff9c4,stroke:#f9a825
    style BACKUP fill:#e3f2fd,stroke:#1565c0
```

---

## üìä Technology Stack Summary

| Layer | Technology | Purpose |
|-------|-----------|---------|
| **OS** | Ubuntu 22.04 LTS | Base VM operating system |
| **Automation** | Ansible | Infrastructure-as-Code, playbook-driven setup |
| **Container Runtime** | containerd | CRI-compliant container runtime |
| **Orchestration** | Kubernetes v1.29 | Container orchestration platform |
| **CNI** | Flannel | Pod networking (VXLAN, low RAM usage) |
| **Monitoring** | Prometheus | Metrics collection, alerting |
| **Visualization** | Grafana | Dashboards, data visualization |
| **Node Metrics** | Node Exporter | Hardware/OS metrics (DaemonSet) |
| **K8s Metrics** | Kube-State-Metrics | Kubernetes object state metrics |
| **Web Server** | Nginx (unprivileged) | Sample workload, PSS-compliant |
| **Storage** | NFS | Shared persistent storage (ReadWriteMany) |
| **Runtime Security** | Falco | Syscall monitoring, threat detection |
| **Firewall** | UFW | Host-level network security |
| **Backup** | etcdctl + cron | Automated etcd snapshots |
| **Security** | CIS Benchmarks | Compliance-aligned hardening |

---

## üöÄ Deployment Flow (End-to-End)

```
1. Configure    ‚Üí  Edit hosts.ini + group_vars/all.yml with your IPs
2. Run Playbook ‚Üí  ansible-playbook -i inventory/hosts.ini site.yml
3. Verify       ‚Üí  kubectl get nodes && kubectl get pods -A
4. Access       ‚Üí  Prometheus :30090 | Grafana :30300 | Nginx :30080
5. Monitor      ‚Üí  Grafana dashboards auto-provisioned with Prometheus data
6. Backup       ‚Üí  etcd snapshots every hour (local + NFS)
7. Diagnose     ‚Üí  ./scripts/diagnose-services.sh (14-point health check)
```
