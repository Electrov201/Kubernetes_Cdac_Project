<p align="center">
  <img src="https://kubernetes.io/images/kubernetes-horizontal-color.png" width="400" alt="Kubernetes Logo"/>
</p>

<h1 align="center">â˜¸ï¸ Kubernetes Cluster Automation</h1>
<h3 align="center">Production-Ready â€¢ Ansible-Powered â€¢ Fully Observable â€¢ CIS-Hardened</h3>

<p align="center">
  <img src="https://img.shields.io/badge/Kubernetes-v1.29-326CE5?style=for-the-badge&logo=kubernetes&logoColor=white" alt="Kubernetes"/>
  <img src="https://img.shields.io/badge/Ansible-Automation-EE0000?style=for-the-badge&logo=ansible&logoColor=white" alt="Ansible"/>
  <img src="https://img.shields.io/badge/Ubuntu-22.04_LTS-E95420?style=for-the-badge&logo=ubuntu&logoColor=white" alt="Ubuntu"/>
  <img src="https://img.shields.io/badge/Prometheus-Monitoring-E6522C?style=for-the-badge&logo=prometheus&logoColor=white" alt="Prometheus"/>
  <img src="https://img.shields.io/badge/Grafana-Dashboards-F46800?style=for-the-badge&logo=grafana&logoColor=white" alt="Grafana"/>
  <img src="https://img.shields.io/badge/Falco-Runtime_Security-00ADEF?style=for-the-badge&logo=falco&logoColor=white" alt="Falco"/>
</p>

<p align="center">
  <b>One command. Full cluster. Zero manual steps.</b><br/>
  A production-grade Kubernetes cluster on Ubuntu 22.04, fully automated with Ansible,<br/>
  featuring monitoring, runtime security, and CIS-hardened infrastructure â€”<br/>
  optimized for an 8GB RAM, 2-node lab environment.
</p>

---

## ğŸ“Œ What This Project Does

```
âŒ WITHOUT this project                     âœ… WITH this project
â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€                  â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
â€¢ 50+ manual commands per node              â€¢ 1 command: ansible-playbook site.yml
â€¢ 2-3 hours of setup time                   â€¢ ~15 mins automated setup  
â€¢ No monitoring by default                  â€¢ Prometheus + Grafana from day 0
â€¢ No security hardening                     â€¢ CIS benchmarks + Falco + UFW
â€¢ Hope nothing breaks                       â€¢ Self-healing + etcd backups
â€¢ "It works on my machine"                  â€¢ 100% reproducible via IaC
```

---

## ğŸ—ï¸ High-Level Architecture

```mermaid
graph TB
    subgraph CONTROL["ğŸ›ï¸ Ansible Control Machine"]
        direction LR
        CMD["$ ansible-playbook site.yml"]
    end

    subgraph CLUSTER["â˜¸ï¸ Kubernetes Cluster v1.29"]
        direction TB
        
        subgraph MASTER["ğŸ–¥ï¸ Master Node â€” 192.168.144.130"]
            direction TB
            API["ğŸ”Œ API Server\n:6443"]
            ETCD["ğŸ’¾ etcd\n:2379"]
            SCHED["ğŸ“‹ Scheduler"]
            CM["ğŸ”„ Controller\nManager"]
        end

        subgraph WORKER["ğŸ–¥ï¸ Worker Node â€” 192.168.144.134"]
            direction TB
            KP["ğŸ”€ kube-proxy"]
            KL["âš™ï¸ kubelet"]
        end

        subgraph NETWORK["ğŸŒ Flannel CNI â€” 10.244.0.0/16 VXLAN Overlay"]
            direction LR
            NET_NOTE["Pod-to-Pod networking across nodes"]
        end
    end

    subgraph SERVICES["ğŸ“¦ Deployed Services"]
        direction TB
        
        subgraph MON["ğŸ“Š Monitoring Stack"]
            direction LR
            PROM["Prometheus\n:30090"]
            GRAF["Grafana\n:30300"]
            NE["Node Exporter\n:9100"]
            KSM["Kube-State\nMetrics :8080"]
        end

        subgraph APP["ğŸš€ Application"]
            NGINX["Nginx x2\n:30080"]
        end

        subgraph SEC["ğŸ›¡ï¸ Security"]
            FALCO["Falco\nDaemonSet"]
            NP["Network\nPolicies"]
            PSS["Pod Security\nStandards"]
        end
    end

    subgraph STORAGE["ğŸ’¾ NFS Server â€” 192.168.144.132"]
        direction TB
        NFS1["/srv/nfs/kubernetes/prometheus â€” 5Gi"]
        NFS2["/srv/nfs/kubernetes/grafana â€” 2Gi"]
        NFS3["/srv/nfs/kubernetes/nginx â€” 1Gi"]
        NFS4["/srv/nfs/etcd-backups â€” 7 days"]
    end

    CMD -->|"SSH â€” Play 1: OS + Security"| MASTER
    CMD -->|"SSH â€” Play 1: OS + Security"| WORKER
    CMD -->|"Play 2: kubeadm init"| MASTER
    CMD -->|"Play 3: kubeadm join"| WORKER
    CMD -->|"Play 4: kubectl apply"| SERVICES

    PROM -->|"scrape /metrics"| NE
    PROM -->|"scrape /metrics"| KSM
    PROM -->|"scrape /metrics"| FALCO
    GRAF -->|"PromQL queries"| PROM

    PROM -.->|"PVC"| NFS1
    GRAF -.->|"PVC"| NFS2
    NGINX -.->|"PVC"| NFS3
    ETCD -.->|"cron backup"| NFS4

    classDef master fill:#4a90d9,stroke:#2d5986,color:#fff,stroke-width:2px
    classDef worker fill:#5cb85c,stroke:#3d8b3d,color:#fff,stroke-width:2px
    classDef monitoring fill:#9b59b6,stroke:#6c3483,color:#fff,stroke-width:2px
    classDef security fill:#e74c3c,stroke:#a93226,color:#fff,stroke-width:2px
    classDef storage fill:#f0ad4e,stroke:#c87f0a,color:#000,stroke-width:2px
    classDef app fill:#17a2b8,stroke:#117a8b,color:#fff,stroke-width:2px

    class API,ETCD,SCHED,CM master
    class KP,KL worker
    class PROM,GRAF,NE,KSM monitoring
    class FALCO,NP,PSS security
    class NFS1,NFS2,NFS3,NFS4 storage
    class NGINX app
```

---

## âš™ï¸ Automation Flow â€” What Happens When You Run `site.yml`

```mermaid
flowchart TD
    START(["â–¶ ansible-playbook -i inventory/hosts.ini site.yml"])
    START --> P1

    subgraph P1["ğŸ”§ Play 1 â€” ALL Nodes: OS Preparation + Security Hardening"]
        direction TB
        P1A["1. Disable swap\n<code>swapoff -a</code>"]
        P1B["2. Install packages\napt-transport-https\nnfs-common, curl"]
        P1C["3. Load kernel modules\n<code>modprobe overlay</code>\n<code>modprobe br_netfilter</code>"]
        P1D["4. Configure sysctl\nip_forward = 1\nbridge-nf-call-iptables = 1"]
        P1E["5. Install containerd\nSet SystemdCgroup = true"]
        P1F["6. Install K8s v1.29\nkubelet + kubeadm + kubectl"]
        P1G["7. UFW Firewall\nDeny incoming by default\nAllow: SSH, 6443, 10250, .."]
        P1H["8. SSH Hardening\nNo root login\nNo password auth"]
        P1I["9. CIS Benchmarks\nFile perms 0600\nDisable anon kubelet auth"]
        
        P1A --> P1B --> P1C --> P1D --> P1E --> P1F --> P1G --> P1H --> P1I
    end

    P1 --> P2

    subgraph P2["ğŸ‘‘ Play 2 â€” MASTER Only: Control Plane Init"]
        direction TB
        P2A["1. kubeadm init\n--pod-network-cidr=10.244.0.0/16\n--cri-socket=containerd"]
        P2B["2. Setup kubectl\nfor root + ubuntu users"]
        P2C["3. Install Flannel CNI\nVXLAN overlay network"]
        P2D["4. Remove NoSchedule taint\nAllow workloads on master"]
        P2E["5. Apply PSS Labels\nenforce: baseline"]
        P2F["6. Generate join command\nfor worker nodes"]
        P2G["7. Setup etcd backup cron\nHourly snapshots â†’ NFS"]

        P2A --> P2B --> P2C --> P2D --> P2E --> P2F --> P2G
    end

    P2 --> P3

    subgraph P3["ğŸ”— Play 3 â€” WORKERS Only: Join Cluster"]
        direction TB
        P3A["1. Get join command\nfrom master via Ansible"]
        P3B["2. kubeadm join\nwith token + CA hash"]
        P3C["3. Wait for node Ready\nPolls kubectl get node"]

        P3A --> P3B --> P3C
    end

    P3 --> P4

    subgraph P4["ğŸ“¦ Play 4 â€” Deploy Services"]
        direction TB
        P4A["1. Copy manifests\nâ†’ /opt/kubernetes/"]
        P4B["2. kubectl apply storage/\nStorageClass + PV + PVC"]
        P4C["3. kubectl apply monitoring/\nPrometheus + Grafana\nNode Exporter + KSM"]
        P4D["4. kubectl apply nginx/\nDeployment + Service"]
        P4E["5. kubectl apply security/\nNetworkPolicy + RBAC"]
        P4F{"Falco\nenabled?"}
        P4G["6. kubectl apply falco/\nNamespace + DaemonSet"]
        P4H["Skip"]

        P4A --> P4B --> P4C --> P4D --> P4E --> P4F
        P4F -->|"Yes"| P4G
        P4F -->|"No"| P4H
    end

    P4G --> DONE
    P4H --> DONE
    DONE(["âœ… Cluster Ready!\nPrometheus :30090\nGrafana :30300\nNginx :30080"])

    style P1 fill:#e8f4f8,stroke:#5dade2,stroke-width:2px
    style P2 fill:#fdebd0,stroke:#f39c12,stroke-width:2px
    style P3 fill:#d5f5e3,stroke:#27ae60,stroke-width:2px
    style P4 fill:#f5eef8,stroke:#8e44ad,stroke-width:2px
    style DONE fill:#d4edda,stroke:#28a745,stroke-width:3px,color:#000
```

---

## ğŸ“Š Monitoring Architecture

```mermaid
graph LR
    subgraph TARGETS["ğŸ¯ Metrics Sources"]
        direction TB
        NE["<b>Node Exporter</b>\n:9100 â€¢ DaemonSet\nCPU, RAM, Disk, Net"]
        KSM["<b>Kube-State-Metrics</b>\n:8080 â€¢ Deployment\nPod/Deploy/Node state"]
        KUBELET["<b>Kubelet</b>\n/metrics via API proxy\nContainer-level metrics"]
        APISVR["<b>API Server</b>\n:6443/metrics\nRequest latency"]
        FALCO_M["<b>Falco</b>\n:8765/metrics\nSecurity event counts"]
    end

    subgraph ENGINE["âš™ï¸ Processing"]
        direction TB
        PROM["<b>Prometheus</b>\n:9090 â†’ NodePort :30090\n\nRetention: 2 days\nStorage: 500MB on NFS\nScrape interval: 30s"]
        ALERTS["<b>Alert Rules</b>\n\nğŸ”´ NodeDown\nğŸŸ¡ HighCPU > 80%\nğŸŸ¡ HighMem > 85%\nğŸŸ¡ DiskLow < 15%\nğŸŸ¡ PodCrashLooping"]
    end

    subgraph VISUAL["ğŸ“º Visualization"]
        direction TB
        GRAF["<b>Grafana</b>\n:3000 â†’ NodePort :30300\nLogin: admin / admin"]
        DASH["<b>Pre-built Dashboards</b>\n\nğŸ“Š Cluster Overview\nğŸ“ˆ Node Metrics\nğŸ“¦ Pod Resources\nğŸ”’ Falco Security"]
    end

    NE -->|"every 30s"| PROM
    KSM -->|"every 30s"| PROM
    KUBELET -->|"every 30s"| PROM
    APISVR -->|"every 30s"| PROM
    FALCO_M -->|"every 30s"| PROM

    ALERTS --> PROM
    PROM --> GRAF
    DASH --> GRAF

    style PROM fill:#e6522c,stroke:#c0392b,color:#fff,stroke-width:2px
    style GRAF fill:#f9a825,stroke:#f57f17,color:#000,stroke-width:2px
    style NE fill:#26a69a,stroke:#00897b,color:#fff
    style KSM fill:#42a5f5,stroke:#1565c0,color:#fff
    style FALCO_M fill:#ef5350,stroke:#c62828,color:#fff
```

---

## ğŸ”’ Security â€” 4-Layer Defense

```mermaid
graph TB
    subgraph L1["<b>ğŸ  Layer 1 â€” Host Security</b><br/>(Ansible security role)"]
        direction LR
        FW["<b>UFW Firewall</b>\nDefault: deny incoming\nWhitelist: SSH, 6443,\n10250, 30000-32767,\n8472/UDP"]
        SSH_H["<b>SSH Hardening</b>\nNo root login\nNo password auth\nKey-only access"]
        KERN["<b>Kernel Hardening</b>\nASLR (randomize_va=2)\nRP filter (anti-spoof)\nNo source routing"]
    end

    subgraph L2["<b>â˜¸ï¸ Layer 2 â€” Kubernetes Security</b><br/>(Pod Security + RBAC)"]
        direction LR
        PSS_D["<b>Pod Security Standards</b>\nenforce: baseline\nBlocks: privileged,\nhostNetwork, hostPath,\ndangerous capabilities"]
        RBAC_D["<b>RBAC</b>\npod-reader Role\ndeveloper RoleBinding\nLeast-privilege access"]
    end

    subgraph L3["<b>ğŸŒ Layer 3 â€” Network Security</b><br/>(NetworkPolicy manifests)"]
        direction LR
        NP1["<b>default-deny-ingress</b>\nBlocks ALL traffic\nto default namespace"]
        NP2["<b>allow-nginx-ingress</b>\nOpens :8080 to\nnginx pods only"]
        NP3["<b>allow-prometheus-scrape</b>\nAllows monitoring NS\nto scrape default NS"]
    end

    subgraph L4["<b>ğŸ›¡ï¸ Layer 4 â€” Runtime Security</b><br/>(Falco DaemonSet)"]
        direction LR
        R1["âš ï¸ Shell Spawned\nin Container"]
        R2["ğŸ”´ Sensitive File\nAccess (/etc/shadow)"]
        R3["ğŸ“ Kubectl Exec\nDetected"]
    end

    subgraph CIS["<b>ğŸ“‹ CIS Kubernetes Benchmarks</b>"]
        direction LR
        C1["CIS 1.1: Manifest perms 0600"]
        C2["CIS 1.1.12: etcd dir 0700"]
        C3["CIS 4.2.1: No anon kubelet"]
        C4["CIS 4.2.4: readOnlyPort = 0"]
    end

    L1 --> L2 --> L3 --> L4
    L1 --> CIS

    style L1 fill:#ffebee,stroke:#c62828,stroke-width:2px
    style L2 fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style L3 fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
    style L4 fill:#fce4ec,stroke:#ad1457,stroke-width:2px
    style CIS fill:#fff3e0,stroke:#e65100,stroke-width:2px
```

---

## ğŸ’¾ Storage Architecture

```mermaid
graph LR
    subgraph NFS["<b>ğŸ–¥ï¸ NFS Server â€” 192.168.144.132</b>"]
        direction TB
        N1["/srv/nfs/kubernetes"]
        N2["/srv/nfs/kubernetes/prometheus"]
        N3["/srv/nfs/kubernetes/grafana"]
        N4["/srv/nfs/kubernetes/nginx"]
        N5["/srv/nfs/etcd-backups"]
    end

    subgraph PVS["<b>ğŸ“¦ PersistentVolumes</b>"]
        direction TB
        PV1["nfs-kubernetes-pv\n10Gi â€¢ RWX"]
        PV2["nfs-prometheus-pv\n5Gi â€¢ RWX"]
        PV3["nfs-grafana-pv\n2Gi â€¢ RWX"]
        PV4["nfs-nginx-pv\n1Gi â€¢ RWX"]
    end

    subgraph PVCS["<b>ğŸ“‹ PersistentVolumeClaims</b>"]
        direction TB
        PVC1["nfs-pvc\n(default ns)"]
        PVC2["prometheus-pvc\n(monitoring ns)"]
        PVC3["grafana-pvc\n(monitoring ns)"]
        PVC4["nginx-pvc\n(default ns)"]
    end

    subgraph PODS["<b>ğŸš€ Consuming Pods</b>"]
        direction TB
        POD1["General Data"]
        POD2["Prometheus"]
        POD3["Grafana"]
        POD4["Nginx"]
    end

    N1 --> PV1 --> PVC1 --> POD1
    N2 --> PV2 --> PVC2 --> POD2
    N3 --> PV3 --> PVC3 --> POD3
    N4 --> PV4 --> PVC4 --> POD4

    N5 -.->|"etcd-backup.sh\nhourly cron"| ETCD_BK["etcd\nsnapshots"]

    style NFS fill:#fff3e0,stroke:#e65100,stroke-width:2px
    style PVS fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style PVCS fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
    style PODS fill:#f3e5f5,stroke:#6a1b9a,stroke-width:2px
```

---

## ğŸŒ Network & Port Map

| Service | Type | Container Port | NodePort | Namespace | Access URL |
|---------|------|:--------------:|:--------:|-----------|------------|
| **Nginx** | NodePort | 8080 | **30080** | default | `http://<node-ip>:30080` |
| **Prometheus** | NodePort | 9090 | **30090** | monitoring | `http://<node-ip>:30090` |
| **Grafana** | NodePort | 3000 | **30300** | monitoring | `http://<node-ip>:30300` |
| **Node Exporter** | ClusterIP | 9100 | â€” | monitoring | Internal only |
| **Kube-State-Metrics** | ClusterIP | 8080 | â€” | monitoring | Internal only |
| **Falco** | ClusterIP | 8765 | â€” | falco | Internal only |

---

## ğŸ“Š Technology Stack

| Layer | Technology | Why This Choice |
|-------|-----------|-----------------|
| **OS** | Ubuntu 22.04 LTS | Long-term support, wide community, ideal for K8s |
| **Automation** | Ansible | Agentless, YAML-based, perfect for server configuration |
| **Container Runtime** | containerd | Official CRI for K8s 1.24+, lighter than Docker (~50MB) |
| **Orchestration** | Kubernetes v1.29 | Industry-standard container orchestration |
| **CNI** | Flannel | Lightweight VXLAN (~50MB RAM), ideal for small clusters |
| **Monitoring** | Prometheus | Pull-based, PromQL, Kubernetes-native service discovery |
| **Visualization** | Grafana | Rich dashboards, multi-datasource, free & open-source |
| **Node Metrics** | Node Exporter | Exposes hardware/OS metrics for Prometheus |
| **K8s Metrics** | Kube-State-Metrics | Exposes Kubernetes object state as metrics |
| **Web Server** | Nginx (unprivileged) | PSS-compliant sample workload with self-healing |
| **Storage** | NFS | ReadWriteMany support, simple, external to cluster |
| **Runtime Security** | Falco | Syscall-level threat detection via eBPF |
| **Firewall** | UFW | Ubuntu-native, simple rule management |
| **Backup** | etcdctl + cron | Automated hourly cluster state snapshots |
| **Compliance** | CIS Benchmarks | Industry-standard security hardening |

---

## ğŸ”„ Self-Healing & Reliability

```mermaid
graph LR
    subgraph PROBES["â¤ï¸ Health Probes"]
        direction TB
        NGINX_P["<b>Nginx</b>\nLiveness: GET / :8080 every 5s\nReadiness: GET / :8080 every 3s\nFail threshold: 3"]
        GRAF_P["<b>Grafana</b>\nLiveness: GET /api/health :3000\nevery 10s\nReadiness: GET /api/health :3000\nevery 5s"]
    end

    subgraph STRATEGY["ğŸ”„ Update Strategy"]
        RS["<b>RollingUpdate</b>\nmaxSurge: 1\nmaxUnavailable: 0\n\nZero-downtime deployments"]
    end

    subgraph BACKUP_S["ğŸ’¾ etcd Backup"]
        direction TB
        CRON_S["Cron: Every hour"]
        SNAP_S["etcdctl snapshot save"]
        LOCAL_S["Local: /backup/etcd\nKeep 24 hours"]
        REMOTE_S["NFS: /mnt/nfs/etcd-backups\nKeep 7 days"]

        CRON_S --> SNAP_S
        SNAP_S --> LOCAL_S
        SNAP_S --> REMOTE_S
    end

    style PROBES fill:#e8f5e9,stroke:#2e7d32,stroke-width:2px
    style STRATEGY fill:#fff9c4,stroke:#f9a825,stroke-width:2px
    style BACKUP_S fill:#e3f2fd,stroke:#1565c0,stroke-width:2px
```

---

## ğŸš€ Quick Start

### Prerequisites

| # | Requirement | Details |
|---|---|---|
| 1 | **Ubuntu 22.04 VMs** | At least 2 â€” one Master, one Worker |
| 2 | **NFS Server** | Ubuntu server with `/srv/nfs/kubernetes` exported |
| 3 | **Ansible** | Installed on your control machine |
| 4 | **SSH Key Access** | Passwordless SSH to all VMs |

### Setup in 4 Steps

```bash
# 1. Clone the repository
git clone https://github.com/Electrov201/Kubernetes_Cdac_Project.git
cd Kubernetes_Cdac_Project

# 2. Update inventory with YOUR VM IPs
nano ansible/inventory/hosts.ini

# 3. Update variables (NFS server IP, etc.)
nano ansible/group_vars/all.yml

# 4. Run the playbook â€” sit back and watch!
cd ansible
ansible-playbook -i inventory/hosts.ini site.yml
```

### âœ… Verify It Worked

```bash
# Check nodes are Ready
kubectl get nodes -o wide

# Check all pods are Running
kubectl get pods --all-namespaces

# Test self-healing (delete a pod, watch it recreate)
kubectl delete pod <nginx-pod-name>
kubectl get pods -w
```

### ğŸ–¥ï¸ Access Services

| Service | URL | Credentials |
|---------|-----|-------------|
| **Prometheus** | `http://<master-ip>:30090` | No login |
| **Grafana** | `http://<master-ip>:30300` | `admin` / `admin` |
| **Nginx** | `http://<master-ip>:30080` | No login |

---

## ğŸ“ Project Structure

```
Kubernetes_Cdac_Project/
â”‚
â”œâ”€â”€ ğŸ“‚ ansible/                              # Infrastructure Automation
â”‚   â”œâ”€â”€ ğŸ“‚ inventory/
â”‚   â”‚   â””â”€â”€ ğŸ“„ hosts.ini                     # Target VM IPs + SSH config
â”‚   â”œâ”€â”€ ğŸ“‚ group_vars/
â”‚   â”‚   â””â”€â”€ ğŸ“„ all.yml                       # All configurable variables
â”‚   â”œâ”€â”€ ğŸ“„ site.yml                          # Main playbook (4 plays)
â”‚   â””â”€â”€ ğŸ“‚ roles/
â”‚       â”œâ”€â”€ ğŸ“‚ common/                       # OS prep, containerd, K8s packages
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ tasks/main.yml            # 13 tasks for node preparation
â”‚       â”‚   â””â”€â”€ ğŸ“„ handlers/main.yml         # Service restart handlers
â”‚       â”œâ”€â”€ ğŸ“‚ k8s_master/                   # Control plane initialization
â”‚       â”‚   â”œâ”€â”€ ğŸ“„ tasks/main.yml            # kubeadm init, CNI, PSS, backup
â”‚       â”‚   â””â”€â”€ ğŸ“„ handlers/main.yml         # Service restart handlers
â”‚       â”œâ”€â”€ ğŸ“‚ k8s_worker/                   # Worker node join
â”‚       â”‚   â””â”€â”€ ğŸ“„ tasks/main.yml            # kubeadm join + readiness wait
â”‚       â””â”€â”€ ğŸ“‚ security/                     # Host-level hardening
â”‚           â”œâ”€â”€ ğŸ“„ tasks/main.yml            # UFW, SSH, CIS benchmarks (34 tasks)
â”‚           â””â”€â”€ ğŸ“„ handlers/main.yml         # SSH + kubelet restart handlers
â”‚
â”œâ”€â”€ ğŸ“‚ kubernetes/                           # K8s Manifests (applied by Ansible)
â”‚   â”œâ”€â”€ ğŸ“‚ monitoring/                       # Observability stack
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ namespace.yaml               # monitoring NS (PSS: baseline)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prometheus.yaml              # RBAC + ConfigMap + Deployment + Service
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ prometheus-alerts.yaml        # Alert rules ConfigMap
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ grafana.yaml                  # Datasource + Deployment + Service
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ grafana-dashboards.yaml       # Pre-built dashboard JSONs
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ kube-state-metrics.yaml       # RBAC + Deployment + Service
â”‚   â”‚   â””â”€â”€ ğŸ“„ node-exporter.yaml            # DaemonSet + Service
â”‚   â”œâ”€â”€ ğŸ“‚ nginx/                            # Sample workload
â”‚   â”‚   â””â”€â”€ ğŸ“„ deployment.yaml              # PSS-compliant Deployment + Service
â”‚   â”œâ”€â”€ ğŸ“‚ security/                         # Cluster security policies
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ network-policy.yaml           # default-deny + allow rules
â”‚   â”‚   â””â”€â”€ ğŸ“„ pss-rbac.yaml                # PSS labels + RBAC role/binding
â”‚   â”œâ”€â”€ ğŸ“‚ storage/                          # Persistent storage (NFS)
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ storage-class.yaml            # nfs-storage StorageClass
â”‚   â”‚   â”œâ”€â”€ ğŸ“„ nfs-pv.yaml                  # 4 PersistentVolumes
â”‚   â”‚   â””â”€â”€ ğŸ“„ nfs-pvc.yaml                 # 4 PersistentVolumeClaims
â”‚   â””â”€â”€ ğŸ“‚ falco/                            # Runtime security (optional)
â”‚       â””â”€â”€ ğŸ“„ falco.yaml                    # NS + RBAC + Config + DaemonSet
â”‚
â”œâ”€â”€ ğŸ“‚ scripts/                              # Operational scripts
â”‚   â”œâ”€â”€ ğŸ“„ etcd-backup.sh                    # Hourly etcd snapshot + NFS copy
â”‚   â””â”€â”€ ğŸ“„ diagnose-services.sh              # 14-point cluster health check
â”‚
â”œâ”€â”€ ğŸ“‚ docs/                                 # Documentation
â”‚   â”œâ”€â”€ ğŸ“„ Kubernetes_Cluster_Project_Document.md  # Complete project documentation
â”‚   â”œâ”€â”€ ğŸ“„ Project_Explanation.md            # Project explanation guide
â”‚   â”œâ”€â”€ ğŸ“„ Interview_QA_Guide.md             # Interview Q&A reference
â”‚   â”œâ”€â”€ ğŸ“„ Updated_Interview_QA.md           # Extended interview guide
â”‚   â”œâ”€â”€ ğŸ“„ interview_extra.md                # Additional interview prep
â”‚   â””â”€â”€ ğŸ“„ setup_guide.md                    # Step-by-step setup instructions
â”‚
â””â”€â”€ ğŸ“„ README.md                             # â† You are here
```

---

## ğŸ”§ Configuration Reference

All settings live in `ansible/group_vars/all.yml`:

| Variable | Default | Description |
|----------|---------|-------------|
| `kubernetes_version` | `1.29` | Kubernetes version to install |
| `api_server_advertise_address` | `192.168.144.130` | Master node IP address |
| `pod_network_cidr` | `10.244.0.0/16` | Pod IP range (must match Flannel) |
| `cni_plugin` | `flannel` | CNI plugin (`flannel` or `calico`) |
| `nfs_server` | `192.168.144.132` | NFS server IP for persistent storage |
| `prometheus_nodeport` | `30090` | Prometheus UI access port |
| `grafana_nodeport` | `30300` | Grafana UI access port |
| `nginx_replicas` | `2` | Nginx pod replicas (optimized for 8GB) |
| `enable_falco` | `true` | Enable runtime security monitoring |
| `pss_level` | `baseline` | Pod Security Standards enforcement level |
| `enable_firewall` | `true` | Enable UFW firewall hardening |
| `allow_master_scheduling` | `true` | Allow pods on master node |

---

## ğŸ“ˆ Scaling Capabilities

| Component | Current | Scaling Method | Notes |
|-----------|:-------:|----------------|-------|
| **Nginx** | 2 replicas | `kubectl scale deployment nginx --replicas=N` | âœ… Manual horizontal scaling |
| **Node Exporter** | DaemonSet | Auto-scales with nodes | âœ… Automatic |
| **Falco** | DaemonSet | Auto-scales with nodes | âœ… Automatic |
| **Prometheus** | 1 replica | Single instance by design | âš ï¸ Thanos needed for HA |
| **Grafana** | 1 replica | Needs shared storage for HA | âš ï¸ NFS already supports it |

> **Note**: This project is optimized for an **8GB RAM, 2-node lab**. For production scaling, add Metrics Server + HPA.

---

## ğŸš€ End-to-End Deployment Flow

```
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  1. CONFIGURE â”‚â”€â”€â”€â”€â–¶â”‚  2. RUN PLAYBOOK  â”‚â”€â”€â”€â”€â–¶â”‚  3. VERIFY     â”‚
   â”‚               â”‚     â”‚                    â”‚     â”‚                â”‚
   â”‚ hosts.ini     â”‚     â”‚ ansible-playbook   â”‚     â”‚ kubectl get    â”‚
   â”‚ all.yml       â”‚     â”‚ site.yml           â”‚     â”‚ nodes && pods  â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                                          â”‚
   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”Œâ”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”
   â”‚  6. BACKUP   â”‚â—€â”€â”€â”€â”€â”‚  5. MONITOR       â”‚â—€â”€â”€â”€â”€â”‚  4. ACCESS     â”‚
   â”‚               â”‚     â”‚                    â”‚     â”‚                â”‚
   â”‚ etcd snapshotsâ”‚     â”‚ Grafana dashboards â”‚     â”‚ :30090 Prom    â”‚
   â”‚ every hour    â”‚     â”‚ auto-provisioned   â”‚     â”‚ :30300 Grafana â”‚
   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚ :30080 Nginx   â”‚
                                                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“š Documentation

| Document | Description |
|----------|-------------|
| [ğŸ“˜ Complete Project Documentation](docs/Kubernetes_Cluster_Project_Document.md) | Full technical deep-dive with "what & why" for every component |
| [ğŸ“– Project Explanation](docs/Project_Explanation.md) | Concise project overview |
| [ğŸ¤ Interview Q&A Guide](docs/Interview_QA_Guide.md) | Interview preparation reference |
| [ğŸ“ Updated Interview Q&A](docs/Updated_Interview_QA.md) | Extended interview guide with scenarios |
| [ğŸ”§ Setup Guide](docs/setup_guide.md) | Step-by-step setup instructions |

---

## ğŸ“‹ Feature Checklist

| Feature | Status | Technology |
|---------|:------:|------------|
| One-command cluster deployment | âœ… | Ansible |
| Container runtime (CRI-compliant) | âœ… | containerd |
| Pod networking (CNI) | âœ… | Flannel VXLAN |
| Metrics collection | âœ… | Prometheus |
| Dashboard visualization | âœ… | Grafana (pre-built dashboards) |
| Host-level metrics | âœ… | Node Exporter (DaemonSet) |
| K8s object metrics | âœ… | Kube-State-Metrics |
| Persistent storage (RWX) | âœ… | NFS PersistentVolumes |
| Firewall hardening | âœ… | UFW (deny-by-default) |
| SSH hardening | âœ… | Key-only, no root login |
| CIS Kubernetes Benchmarks | âœ… | Ansible security role |
| Pod Security Standards | âœ… | Baseline enforcement |
| Network Policies | âœ… | Zero-trust (default deny) |
| RBAC | âœ… | Least-privilege roles |
| Runtime threat detection | âœ… | Falco (syscall monitoring) |
| Self-healing workloads | âœ… | Liveness + Readiness probes |
| Rolling updates | âœ… | Zero-downtime deployments |
| Automated backups | âœ… | etcd hourly snapshots |
| Cluster diagnostics | âœ… | 14-point health check script |
| Alert rules | âœ… | Prometheus alerting |

---

<p align="center">
  <b>Built with â¤ï¸ as part of the CDAC program</b><br/>
  <i>Demonstrating production-grade DevOps practices in a resource-constrained environment</i>
</p>
