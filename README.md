# Kubernetes Cluster Setup with Ansible Automation

A production-ready Kubernetes cluster setup with monitoring, security, and automation.

## ğŸš€ Quick Start (8GB RAM Setup)

### Prerequisites

1. **Two Ubuntu 22.04 VMs** (4GB RAM each)
2. **Ansible installed** on your control machine
3. **SSH key access** to both VMs

### Setup Steps

```bash
# 1. Clone/Copy project to your Ansible control machine
cd "Cdac Project"

# 2. Update inventory with your VM IPs
nano ansible/inventory/hosts.ini

# 3. Update variables (NFS server, etc.)
nano ansible/group_vars/all.yml

# 4. Run the playbook
cd ansible
ansible-playbook -i inventory/hosts.ini site.yml
```

## ğŸ“ Project Structure

```
Cdac Project/
â”œâ”€â”€ ansible/
â”‚   â”œâ”€â”€ inventory/hosts.ini     # Update with your VM IPs
â”‚   â”œâ”€â”€ group_vars/all.yml      # Configuration variables
â”‚   â”œâ”€â”€ site.yml                # Main playbook
â”‚   â””â”€â”€ roles/
â”‚       â”œâ”€â”€ common/             # Prerequisites & containerd
â”‚       â”œâ”€â”€ k8s_master/         # Control plane setup
â”‚       â”œâ”€â”€ k8s_worker/         # Worker node join
â”‚       â””â”€â”€ security/           # Firewall & hardening
â”œâ”€â”€ kubernetes/
â”‚   â”œâ”€â”€ monitoring/             # Prometheus & Grafana
â”‚   â”œâ”€â”€ storage/                # NFS PV/PVC
â”‚   â”œâ”€â”€ nginx/                  # Sample workload
â”‚   â”œâ”€â”€ security/               # Network Policies & RBAC
â”‚   â””â”€â”€ falco/                  # Runtime security (optional)
â”œâ”€â”€ scripts/
â”‚   â””â”€â”€ etcd-backup.sh         # Automated backup script
â””â”€â”€ docs/
    â””â”€â”€ Kubernetes_Cluster_Project_Document.md
```

## ğŸ”§ Configuration

Edit `ansible/group_vars/all.yml`:

| Variable | Description | Default |
|----------|-------------|---------|
| `api_server_advertise_address` | Master node IP | 192.168.1.10 |
| `nfs_server` | TrueNAS IP | 192.168.1.100 |
| `cni_plugin` | flannel or calico | flannel |
| `enable_falco` | Enable runtime security | false |

## ğŸ–¥ï¸ Access Services

After deployment:

| Service | URL | Credentials |
|---------|-----|-------------|
| Prometheus | http://<node-ip>:30090 | N/A |
| Grafana | http://<node-ip>:30300 | admin / admin |
| Nginx | http://<node-ip>:30080 | N/A |

## âœ… Verification

```bash
# Check nodes
kubectl get nodes

# Check pods
kubectl get pods --all-namespaces

# Test self-healing
kubectl delete pod <nginx-pod-name>
kubectl get pods -w
```

## ğŸ“š Documentation

See [Kubernetes_Cluster_Project_Document.md](docs/Kubernetes_Cluster_Project_Document.md) for complete documentation.

## ğŸ“‹ Features

- âœ… **Automation**: Ansible-based deployment
- âœ… **Monitoring**: Prometheus + Grafana
- âœ… **Security**: PSS, Network Policies, RBAC, Firewall
- âœ… **Storage**: TrueNAS NFS integration
- âœ… **Self-Healing**: Liveness/Readiness probes
- âœ… **Backup**: Automated etcd backup
- âœ… **Runtime Security**: Falco (optional)

## âš ï¸ 8GB RAM Notes

This setup is optimized for 8GB total RAM:
- Uses Flannel CNI (lighter than Calico)
- Master taint removed (runs workloads)
- Resource limits on all pods
- Falco enabled with resource limits (256MB per node)

## ğŸ“ License

MIT License - Free for educational use
